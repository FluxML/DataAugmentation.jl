var documenterSearchIndex = {"docs":
[{"location":"preprocessing/#Preprocessing","page":"Preprocessing","title":"Preprocessing","text":"","category":"section"},{"location":"preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"This library also implements some general transformations useful for getting data ready to be put into a model.","category":"page"},{"location":"preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"ToEltype(T) converts the element type of any DataAugmentation.AbstractArrayItem to T.\nImageToTensor converts an image to an ArrayItem with another dimension for the color channels \nNormalize normalizes image tensors\nOneHot to one-hot encode multi-class masks (MaskMultis)","category":"page"},{"location":"projective/data/#Spatial-data","page":"Data","title":"Spatial data","text":"","category":"section"},{"location":"projective/data/","page":"Data","title":"Data","text":"Before introducing various projective transformations, we have a look at the data that can be projected. There are three kinds of data currently supported: images, keypoints and segmentation masks. Both 2D and 3D data is supported (and technically, higher dimensions, but I've yet to find a dataset with 4 spatial dimensions).","category":"page"},{"location":"projective/data/","page":"Data","title":"Data","text":"Image{N, T} represents an N-dimensional image. T refers to the element type of the array that Image wraps, usually a color. When projecting images, proper interpolation methods are used to reduce artifacts like aliasing. See src/items/image.jl\nMaskBinary{N} and MaskMulti{N, T} likewise represents N-dimensional segmentation masks. Unlike images, nearest-neighbor interpolation is used for projecting masks. See src/items/mask.jl\nLastly, Keypoints{N} represent keypoint data. The data should be an array of SVector{N}. Since there are many interpretations of keypoint data, there are also wrapper items for convenience: BoundingBox and Polygon.See src/items/keypoints.jl","category":"page"},{"location":"projective/gallery/#Gallery","page":"Gallery","title":"Gallery","text":"","category":"section"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"Let's visualize what these projective transformations look like.","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"You can apply them to Images and the keypoint-based items Keypoints, Polygon, and BoundingBox.","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"Let's take this picture of a light house:","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"using DataAugmentation\nusing MosaicViews\nusing Images\nusing TestImages\nusing StaticArrays\n\nimagedata = testimage(\"lighthouse\")\nimagedata = imresize(imagedata, ratio = 196 / size(imagedata, 1))","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"To apply a transformation tfm to it, wrap it in Image, apply the transformation and unwrap it using itemdata:","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"tfm = CenterCrop((196, 196))\nimage = Image(imagedata)\napply(tfm, image) |> itemdata","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"Now let's say we want to train a light house detector and have a bounding box for the light house. We can use the BoundingBox item to represent it. It takes the two corners of the bounding rectangle as the first argument. As the second argument we have to pass the size of the corresponding image.","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"points = SVector{2, Float32}[SVector(23., 120.), SVector(120., 150.)]\nbbox = BoundingBox(points, size(imagedata))","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"showitems visualizes the two items:","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"showitems((image, bbox))","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"If we apply transformations like translation and cropping to the image, then the same transformations have to be applied to the bounding box. Otherwise, the bounding box will no longer match up with the light house.","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"Another problem can occur with stochastic transformations like RandomResizeCrop If we apply it separately to the image and the bounding box, they will be cropped from slightly different locations:","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"tfm = RandomResizeCrop((128, 128))\nshowitems((\n    apply(tfm, image),\n    apply(tfm, bbox)\n))","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"Instead, pass a tuple of the items to a single apply call so the same random state will be used for both image and bounding box:","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"apply(tfm, (image, bbox)) |> showitems","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"info: 3D Projective dimensions\nWe'll use a 2-dimensional Image and BoundingBox here, but you can apply most projective transformations to any spatial item (including Keypoints, MaskBinary and MaskMulti) in 3 dimensions.Of course, you have to create a 3-dimensional transformation, i.e. CenterCrop((128, 128, 128)) instead of CenterCrop((128, 128)).","category":"page"},{"location":"projective/gallery/#[RandomResizeCrop](@ref)(sz)","page":"Gallery","title":"RandomResizeCrop(sz)","text":"","category":"section"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"Resizes the sides so that one of them is no longer than sz and crops a region of size sz from a random location.","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"tfm = RandomResizeCrop((128, 128))\nshowgrid([apply(tfm, (image, bbox)) for _ in 1:6]; ncol=6, npad=8)","category":"page"},{"location":"projective/gallery/#[CenterResizeCrop](@ref)","page":"Gallery","title":"CenterResizeCrop","text":"","category":"section"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"Resizes the sides so that one of them is no longer than sz and crops a region of size sz from the center.","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"tfm = CenterResizeCrop((128, 128))\nshowgrid([apply(tfm, (image, bbox))]; ncol=6, npad=8)","category":"page"},{"location":"projective/gallery/#[Crop](@ref)(sz[,-from])","page":"Gallery","title":"Crop(sz[, from])","text":"","category":"section"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"Crops a region of size sz from the image, without resizing the image first.","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"using DataAugmentation: FromOrigin, FromCenter, FromRandom\ntfms = [\n    Crop((128, 128), FromOrigin()),\n    Crop((128, 128), FromCenter()),\n    Crop((128, 128), FromRandom()),\n    Crop((128, 128), FromRandom()),\n    Crop((128, 128), FromRandom()),\n    Crop((128, 128), FromRandom()),\n]\nshowgrid([apply(tfm, (image, bbox)) for tfm in tfms]; ncol=6, npad=8)","category":"page"},{"location":"projective/gallery/#[FlipX](@ref),-[FlipY](@ref),-[Reflect](@ref)","page":"Gallery","title":"FlipX, FlipY, Reflect","text":"","category":"section"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"Flip the data on the horizontally and vertically, respectively. More generally, reflect around an angle from the x-axis.","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"tfms = [\n    FlipX(),\n    FlipY(),\n    Reflect(30),\n]\nshowgrid([apply(tfm, (image, bbox)) for tfm in tfms]; ncol=6, npad=8)","category":"page"},{"location":"projective/gallery/#[Rotate](@ref),-[RotateX](@ref),-[RotateY](@ref),-[RotateZ](@ref)","page":"Gallery","title":"Rotate, RotateX, RotateY, RotateZ","text":"","category":"section"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"Rotate a 2D image counter-clockwise by an angle.","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"tfm = Rotate(20) |> CenterCrop((256, 256))\nshowgrid([apply(tfm, (image, bbox)) for _ in 1:6]; ncol=6, npad=8)","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"Rotate also works with 3D images in addition to 3D specific transforms RotateX, RotateY, and RotateZ.","category":"page"},{"location":"projective/gallery/","page":"Gallery","title":"Gallery","text":"image3D = Image([RGB(i, j, k) for i=0:0.01:1, j=0:0.01:1, k=0:0.01:1])\ntfms = [\n    Rotate(20, 30, 40),\n    Rotate{3}(45),\n    RotateX(45),\n    RotateY(45),\n    RotateZ(45),\n]\ntransformed = [apply(tfm, image3D) |> itemdata for tfm in tfms]\nslices = [Image(parent(t[:, :, 50])) for t in transformed]\nshowgrid(slices; ncol=6, npad=8)","category":"page"},{"location":"tfminterface/#Transformation-interface","page":"Transform Interface","title":"Transformation interface","text":"","category":"section"},{"location":"tfminterface/","page":"Transform Interface","title":"Transform Interface","text":"{style=\"opacity:60%;\"} src/base.jl","category":"page"},{"location":"tfminterface/","page":"Transform Interface","title":"Transform Interface","text":"The transformation interface is the centrepiece of this library. Beside straightforward transform application it also enables stochasticity, composition and buffering.","category":"page"},{"location":"tfminterface/","page":"Transform Interface","title":"Transform Interface","text":"A transformation is a type that subtypes Transform. The only required function to implement for your transformation type T is","category":"page"},{"location":"tfminterface/","page":"Transform Interface","title":"Transform Interface","text":"apply(tfm::T, item::I; randstate)\nApplies the transformation tfm to item item. Implemented methods can of course dispatch on the type of item. randstate encapsulates the random state needed for stochastic transformations. The apply method implementation itself should be deterministic.\nYou may dispatch on a specific item type I or use the abstract Item if one implementation works for all item types.","category":"page"},{"location":"tfminterface/","page":"Transform Interface","title":"Transform Interface","text":"You may additionally also implement:","category":"page"},{"location":"tfminterface/","page":"Transform Interface","title":"Transform Interface","text":"DataAugmentation.getrandstate(tfm) for stochastic transformations\nGenerates random state to be used inside apply. Calling apply(tfm, item) is equivalent to   apply(tfm, item; randstate = getrandstate(tfm)). It defaults to nothing, so we need not implement it for deterministic transformations.\napply!(bufitem, tfm::T, item; randstate) to support buffering\nBuffered version of apply that mutates bufitem. If not implemented,   falls back to regular apply.\nDataAugmentation.compose(tfm1, tfm2) for custom composition with other transformations\nComposes transformations. By default, returns a Sequence transformation that applies the transformations one after the other.","category":"page"},{"location":"tfminterface/#Example","page":"Transform Interface","title":"Example","text":"","category":"section"},{"location":"tfminterface/","page":"Transform Interface","title":"Transform Interface","text":"The implementation of the MapElem transformation illustrates this interface well. It transforms any item with array data by mapping a function over the array's elements, just like Base.map.","category":"page"},{"location":"tfminterface/","page":"Transform Interface","title":"Transform Interface","text":"struct MapElem <: Transform\n    f\nend","category":"page"},{"location":"tfminterface/","page":"Transform Interface","title":"Transform Interface","text":"The apply implementation dispatches on DataAugmentation.AbstractArrayItem, an abstract item type for items that wrap arrays. Note that the randstate keyword argument needs to be given even for implementations of deterministic transformations. We also make use of the DataAugmentation.setdata helper to update the item data.","category":"page"},{"location":"tfminterface/","page":"Transform Interface","title":"Transform Interface","text":"function apply(tfm::MapElem, item::AbstractArrayItem; randstate = nothing)\n    a = itemdata(item)\n    a_ = map(tfm.f, a)\n    return setdata(item, a_)\nend","category":"page"},{"location":"tfminterface/","page":"Transform Interface","title":"Transform Interface","text":"The buffered version applies the function inplace using Base.map!:","category":"page"},{"location":"tfminterface/","page":"Transform Interface","title":"Transform Interface","text":"function apply!(\n        bufitem::I,\n        tfm::MapElem,\n        item::I;\n        randstate = nothing) where I <: AbstractArrayItem\n    map!(tfm.f, itemdata(bufitem), itemdata(item))\n    return bufitem\nend","category":"page"},{"location":"tfminterface/","page":"Transform Interface","title":"Transform Interface","text":"Finally, a MapElem can also be composed nicely with other MapElems. Instead of applying them sequentially, the functions are fused and applied once.","category":"page"},{"location":"tfminterface/","page":"Transform Interface","title":"Transform Interface","text":"compose(tfm1::MapElem, tfm2::MapElem2) = MapElem(tfm2.f ∘ tfm1.f)","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"using DataAugmentation","category":"page"},{"location":"transformations/#Usage","page":"Transformations","title":"Usage","text":"","category":"section"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"Using  transformations is easy. Simply compose them:","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"tfm = Rotate(10) |> ScaleRatio((0.7,0.1,1.2)) |> FlipX() |> Crop((128, 128))","category":"page"},{"location":"transformations/#Projective-transformations","page":"Transformations","title":"Projective transformations","text":"","category":"section"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"DataAugmentation.jl has great support for transforming spatial data like images and keypoints. Most of these transformations are projective transformations. For our purposes, a projection means a mapping between two coordinate spaces. In computer vision, these are frequently used for preprocessing and augmenting image data: images are randomly scaled, maybe flipped horizontally and finally cropped to the same size.","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"This library generalizes projective transformations for different kinds of image and keypoint data in an N-dimensional Euclidean space. It also uses composition for performance improvements like fusing affine transformations.","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"Unlike mathematical objects, the spatial data we want to transform has spatial bounds. For an image, these bounds are akin to the array size. But keypoint data aligned with an image has the same bounds even if they are not explicitly encoded in the representation of the data. These spatial bounds can be used to dynamically create useful transformations. For example, a rotation around the center or a horizontal flip of keypoint annotations can be calculated from the bounds.","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"Often, we also want to crop an area from the projected results. By evaluating only the parts of a projection that fall inside the cropped area, a lot of unnecessary computation can be avoided.","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"Projective transformations include:","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"Affine transformations\nCrops","category":"page"},{"location":"transformations/#Affine-transformations","page":"Transformations","title":"Affine transformations","text":"","category":"section"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"Affine transformations are a subgroup of projective transformations that can be composed very efficiently: composing two affine transformations results in another affine transformation. Affine transformations can represent translation, scaling, reflection and rotation. Available Transforms are:","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"FlipX\nFlipY\nReflect\nRotate\nRotateX\nRotateY\nRotateZ\nScaleKeepAspect\nScaleFixed\nScaleRatio\nWarpAffine\nZoom","category":"page"},{"location":"transformations/#DataAugmentation.FlipX-transformations","page":"Transformations","title":"DataAugmentation.FlipX","text":"Reflect(180)\n\n\n\n\n\n","category":"function"},{"location":"transformations/#DataAugmentation.FlipY-transformations","page":"Transformations","title":"DataAugmentation.FlipY","text":"Reflect(90)\n\n\n\n\n\n","category":"function"},{"location":"transformations/#DataAugmentation.Reflect-transformations","page":"Transformations","title":"DataAugmentation.Reflect","text":"Reflect(γ)\nReflect(distribution)\n\nReflect 2D spatial data around the center by an angle chosen at uniformly from [-γ, γ], an angle given in degrees.\n\nYou can also pass any Distributions.Sampleable from which the angle is selected.\n\nExamples\n\ntfm = Reflect(10)\n\n\n\n\n\n","category":"type"},{"location":"transformations/#DataAugmentation.Rotate-transformations","page":"Transformations","title":"DataAugmentation.Rotate","text":"Rotate(γ)\nRotate(distribution)\nRotate(α, β, γ)\nRotate(α_distribution, β_distribution, γ_distribution)\n\nRotate spatial data around its center. Rotate(γ) is a 2D rotation by an angle chosen uniformly from [-γ, γ], an angle given in degrees. Rotate(α, β, γ) is a 3D rotation by angles chosen uniformly from [-α, α], [-β, β], and [-γ, γ], for X, Y, and Z rotations.\n\nYou can also pass any Distributions.Sampleable from which the angle is selected.\n\nExamples\n\ntfm2d = Rotate(10)\napply(tfm2d, Image(rand(Float32, 16, 16)))\n\ntfm3d = Rotate(10, 20, 30)\napply(tfm3d, Image(rand(Float32, 16, 16, 16)))\n\n\n\n\n\n","category":"type"},{"location":"transformations/#DataAugmentation.RotateX-transformations","page":"Transformations","title":"DataAugmentation.RotateX","text":"RotateX(γ)\nRotateX(distribution)\n\nX-Axis rotation of 3D spatial data around the center by an angle chosen uniformly from [-γ, γ], an angle given in degrees.\n\nYou can also pass any Distributions.Sampleable from which the angle is selected.\n\n\n\n\n\n","category":"function"},{"location":"transformations/#DataAugmentation.RotateY-transformations","page":"Transformations","title":"DataAugmentation.RotateY","text":"RotateY(γ)\nRotateY(distribution)\n\nY-Axis rotation of 3D spatial data around the center by an angle chosen uniformly from [-γ, γ], an angle given in degrees.\n\nYou can also pass any Distributions.Sampleable from which the angle is selected.\n\n\n\n\n\n","category":"function"},{"location":"transformations/#DataAugmentation.RotateZ-transformations","page":"Transformations","title":"DataAugmentation.RotateZ","text":"RotateZ(γ)\nRotateZ(distribution)\n\nZ-Axis rotation of 3D spatial data around the center by an angle chosen uniformly from [-γ, γ], an angle given in degrees.\n\nYou can also pass any Distributions.Sampleable from which the angle is selected.\n\n\n\n\n\n","category":"function"},{"location":"transformations/#DataAugmentation.ScaleKeepAspect-transformations","page":"Transformations","title":"DataAugmentation.ScaleKeepAspect","text":"ScaleKeepAspect(minlengths) <: ProjectiveTransform\n\nScales the shortest side of item to minlengths, keeping the original aspect ratio.\n\nExamples\n\nusing DataAugmentation, TestImages\nimage = testimage(\"lighthouse\")\ntfm = ScaleKeepAspect((200, 200))\napply(tfm, Image(image))\n\n\n\n\n\n","category":"type"},{"location":"transformations/#DataAugmentation.ScaleFixed-transformations","page":"Transformations","title":"DataAugmentation.ScaleFixed","text":"ScaleFixed(sizes)\n\nProjective transformation that scales sides to sizes, disregarding aspect ratio.\n\nSee also ScaleKeepAspect.\n\n\n\n\n\n","category":"type"},{"location":"transformations/#DataAugmentation.ScaleRatio-transformations","page":"Transformations","title":"DataAugmentation.ScaleRatio","text":"ScaleRatio(minlengths) <: ProjectiveTransform\n\nScales the aspect ratio\n\n\n\n\n\n","category":"type"},{"location":"transformations/#DataAugmentation.WarpAffine-transformations","page":"Transformations","title":"DataAugmentation.WarpAffine","text":"WarpAffine(σ = 0.1) <: ProjectiveTransform\n\nA three-point affine warp calculated by randomly moving 3 corners of an item. Similar to a random translation, shear and rotation.\n\n\n\n\n\n","category":"type"},{"location":"transformations/#DataAugmentation.Zoom-transformations","page":"Transformations","title":"DataAugmentation.Zoom","text":"Zoom(scales = (1, 1.2)) <: ProjectiveTransform\nZoom(distribution)\n\nZoom into an item by a factor chosen from the interval scales or distribution.\n\n\n\n\n\n","category":"type"},{"location":"transformations/#Crops","page":"Transformations","title":"Crops","text":"","category":"section"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"To get a cropped result, simply compose any ProjectiveTransform with","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"CenterCrop\nRandomCrop","category":"page"},{"location":"transformations/#DataAugmentation.CenterCrop-transformations","page":"Transformations","title":"DataAugmentation.CenterCrop","text":"Crop(sz, FromCenter())\n\n\n\n\n\n","category":"function"},{"location":"transformations/#DataAugmentation.RandomCrop-transformations","page":"Transformations","title":"DataAugmentation.RandomCrop","text":"Crop(sz, FromRandom())\n\n\n\n\n\n","category":"function"},{"location":"transformations/#Color-transformations","page":"Transformations","title":"Color transformations","text":"","category":"section"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"DataAugmentation.jl currently supports the following color transformations for augmentation:","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"AdjustBrightness\nAdjustContrast","category":"page"},{"location":"transformations/#DataAugmentation.AdjustBrightness-transformations","page":"Transformations","title":"DataAugmentation.AdjustBrightness","text":"AdjustBrightness(δ = 0.2)\nAdjustBrightness(distribution)\n\nAdjust the brightness of an image by a factor chosen uniformly from f ∈ [1-δ, 1+δ] by multiplying each color channel by f.\n\nYou can also pass any Distributions.Sampleable from which the factor is selected.\n\nPixels are clamped to [0,1] unless clamp=false is passed.\n\nExample\n\nusing DataAugmentation, TestImages\n\nitem = Image(testimage(\"lighthouse\"))\ntfm = AdjustBrightness(0.2)\ntitems = [apply(tfm, item) for _ in 1:8]\nshowgrid(titems; ncol = 4, npad = 16)\n\n\n\n\n\n","category":"type"},{"location":"transformations/#DataAugmentation.AdjustContrast-transformations","page":"Transformations","title":"DataAugmentation.AdjustContrast","text":"AdjustContrast(factor = 0.2)\nAdjustContrast(distribution)\n\nAdjust the contrast of an image by a factor chosen uniformly from f ∈ [1-δ, 1+δ].\n\nPixels c are transformed c + μ*(1-f) where μ is the mean color of the image.\n\nYou can also pass any Distributions.Sampleable from which the factor is selected.\n\nPixels are clamped to [0,1] unless clamp=false is passed.\n\nExample\n\nusing DataAugmentation, TestImages\n\nitem = Image(testimage(\"lighthouse\"))\ntfm = AdjustContrast(0.2)\ntitems = [apply(tfm, item) for _ in 1:8]\nshowgrid(titems; ncol = 4, npad = 16)\n\n\n\n\n\n","category":"type"},{"location":"transformations/#Stochastic-transformations","page":"Transformations","title":"Stochastic transformations","text":"","category":"section"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"When augmenting data, it is often useful to apply a transformation only with some probability or choose from a set of transformations. Unlike in other data augmentation libraries like albumentations, in DataAugmentation.jl you can use wrapper transformations for this functionality.","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"Maybe(tfm, p = 0.5) applies a transformation with probability p; and\nOneOf([tfm1, tfm2]) randomly selects a transformation to apply.","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"Maybe\nOneOf","category":"page"},{"location":"transformations/#DataAugmentation.Maybe-transformations","page":"Transformations","title":"DataAugmentation.Maybe","text":"Maybe(tfm, p = 0.5) <: Transform\n\nWith probability p, apply transformation tfm.\n\n\n\n\n\n","category":"function"},{"location":"transformations/#DataAugmentation.OneOf-transformations","page":"Transformations","title":"DataAugmentation.OneOf","text":"OneOf(tfms)\nOneOf(tfms, ps)\n\nApply one of tfms selected randomly with probability ps each or uniformly chosen if no ps is given.\n\n\n\n\n\n","category":"type"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"Let's say we have an image classification dataset. For most datasets, horizontally flipping the image does not change the label: a flipped image of a cat still shows a cat. So let's flip every image horizontally half of the time to improve the generalization of the model we might be training.","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"using DataAugmentation, TestImages\nitem = Image(testimage(\"lighthouse\"))\ntfm = Maybe(FlipX())\ntitems = [apply(tfm, item) for _ in 1:8]\nshowgrid(titems; ncol = 4, npad = 16)","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"DataAugmentation.ImageToTensor","category":"page"},{"location":"transformations/#DataAugmentation.ImageToTensor-transformations","page":"Transformations","title":"DataAugmentation.ImageToTensor","text":"ImageToTensor()\n\nExpands an Image{N, T} of size (height, width, ...) to an ArrayItem{N+1} with size (width, height, ..., ch) where ch is the number of color channels of T.\n\nSupports apply!.\n\nExamples\n\n{cell=ImageToTensor}\n\nusing DataAugmentation, Images\n\nh, w = 40, 50\nimage = Image(rand(RGB, h, w))\ntfm = ImageToTensor()\napply(tfm, image) # ArrayItem in WHC format of size (50, 40, 3)\n\n\n\n\n\n","category":"type"},{"location":"buffering/#Buffering","page":"Buffering","title":"Buffering","text":"","category":"section"},{"location":"buffering/","page":"Buffering","title":"Buffering","text":"As mentioned in the section on transformations, you can implement apply! as an inplace version of apply to support buffered transformations. Usually, the result of a regular apply can be used as a buffer. You may write","category":"page"},{"location":"buffering/","page":"Buffering","title":"Buffering","text":"buffer = apply(tfm, item)\napply!(buffer, tfm, item)","category":"page"},{"location":"buffering/","page":"Buffering","title":"Buffering","text":"However, for some transformations, a different buffer is needed. Sequence, for example, needs to reuse all intermediate results. That is why the buffer creation can be customized:","category":"page"},{"location":"buffering/","page":"Buffering","title":"Buffering","text":"DataAugmentation.makebuffer(tfm, item) creates a buffer buf that can be used in an apply! call: apply!(buf, tfm, item).","category":"page"},{"location":"buffering/","page":"Buffering","title":"Buffering","text":"","category":"page"},{"location":"buffering/","page":"Buffering","title":"Buffering","text":"Managing the buffers manually quickly becomes tedious. For convenience, this library implements DataAugmentation.Buffered, a transformation wrapper that will use a buffer internally. btfm = Buffered(tfm) will create a buffer the first time it is applyed and then use it by internally calling apply!.","category":"page"},{"location":"buffering/","page":"Buffering","title":"Buffering","text":"buffered = Buffered(tfm)\nbuffer = apply(tfm, item)  # uses apply! internally","category":"page"},{"location":"buffering/","page":"Buffering","title":"Buffering","text":"Since Buffered only stores one buffer, you may run into problems when using it in a multi-threading context where different threads invalidate the buffer before it can be used. In that case, you can use DataAugmentation.BufferedThreadsafe, a version of Buffered that keeps a separate buffer for every thread. ","category":"page"},{"location":"buffering/","page":"Buffering","title":"Buffering","text":"DataAugmentation.Buffered\nDataAugmentation.BufferedThreadsafe","category":"page"},{"location":"buffering/#DataAugmentation.Buffered-buffering","page":"Buffering","title":"DataAugmentation.Buffered","text":"Buffer to store transform results\n\n\n\n\n\n","category":"type"},{"location":"buffering/#DataAugmentation.BufferedThreadsafe-buffering","page":"Buffering","title":"DataAugmentation.BufferedThreadsafe","text":"Buffer to store transform results (threadsafe)\n\n\n\n\n\n","category":"type"},{"location":"projective/intro/#Projective-transformations","page":"Intro","title":"Projective transformations","text":"","category":"section"},{"location":"projective/intro/#An-example-pipeline","page":"Intro","title":"An example pipeline","text":"","category":"section"},{"location":"projective/intro/","page":"Intro","title":"Intro","text":"We can break down most augmentation used in practive into a single (possibly stochastic) projection and a crop.","category":"page"},{"location":"projective/intro/","page":"Intro","title":"Intro","text":"As an example, consider an image augmentation pipeline: A random horizontal flip, followed by a random resized crop. The latter resizes and crops (irregularly sized) images to a common size without distorting the aspect ratio.","category":"page"},{"location":"projective/intro/","page":"Intro","title":"Intro","text":"Maybe(FlipX()) |> RandomResizeCrop((h, w))","category":"page"},{"location":"projective/intro/","page":"Intro","title":"Intro","text":"Let's pull apart the steps involved. ","category":"page"},{"location":"projective/intro/","page":"Intro","title":"Intro","text":"Half of the time, flip the image horizontally.\nScale the image down without distortion so that the shorter side length is 128. With an input of size (512, 256) this result in scaling both dimensions by 1/2, resulting in an image with side lengths (256, 128).\nCrop a random (128, 128) portion from that image. There is only \"wiggle room\" on the y-axis (which, by convention, is the first).","category":"page"},{"location":"projective/intro/","page":"Intro","title":"Intro","text":"All of these steps can be efficiently computed in one step with two tricks:","category":"page"},{"location":"projective/intro/","page":"Intro","title":"Intro","text":"Some projections like reflection, translation, scaling and rotation can be composed into a single projection matrix. This means in the above example we only need to apply one projection which represents both the flipping (a reflection) and the scaling. Especially in pipelines with many augmentation steps this avoids a lot of unnecessary computation.\nIn cases, where the result of the projection is cropped, we can save additional computing by only evaluating the parts that we want to keep. ","category":"page"},{"location":"projective/intro/#Cropping","page":"Intro","title":"Cropping","text":"","category":"section"},{"location":"projective/intro/","page":"Intro","title":"Intro","text":"By default, the bounds of a projected item will be chosen so they still encase all the data. So after applying a Scale((2, 2)) to an Image, its bounds will also be scaled by 2. Sometimes, however, we want to crop a part of the projected output, for example so a number of images can later be batched into a single array. While the crop usually has a fixed size, the region to crop still needs to be chosen. For validation data (which should be transformed deterministically), a center crop is usually used. For training data, on the other hand, a random region is selected to add additional augmentation. ","category":"page"},{"location":"projective/intro/#Projective-transformations-interface","page":"Intro","title":"Projective transformations interface","text":"","category":"section"},{"location":"projective/intro/","page":"Intro","title":"Intro","text":"The abstract type DataAugmentation.ProjectiveTransform represents a projective transformation. A ProjectiveTransform needs to implement DataAugmentation.getprojection(tfm, bounds; randstate) that should return a Transformation from CoordinateTransformations.jl.","category":"page"},{"location":"projective/intro/","page":"Intro","title":"Intro","text":"To add support for projective transformations to an item I, you need to implement","category":"page"},{"location":"projective/intro/","page":"Intro","title":"Intro","text":"getbounds(item::I) returns the spatial bounds of the item; and\nproject(P, item::I, indices) applies the projective transformation P and crops to indices","category":"page"},{"location":"projective/intro/","page":"Intro","title":"Intro","text":"To support apply!-ing projective transformations, project!(bufitem, P, item) can also be implemented.","category":"page"},{"location":"quickstart/#Quickstart","page":"Quickstart","title":"Quickstart","text":"","category":"section"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"using TestImages\nusing ImageShow\nusing Images\nusing DataAugmentation","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Import the library:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"using DataAugmentation  # Image, CenterResizeCrop, apply, showitem\nusing TestImages: testimage","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Load your data:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"image = testimage(\"lighthouse\")","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Create an item that contains the data you want to augment:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"item = Image(image)","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Create a transform:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"tfm = CenterResizeCrop((128, 128))","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Apply the transformation and unwrap the data:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"titem = apply(tfm, item)\ntimage = itemdata(titem)","category":"page"},{"location":"iteminterface/#Item-interface","page":"Item Interface","title":"Item interface","text":"","category":"section"},{"location":"iteminterface/","page":"Item Interface","title":"Item Interface","text":"As described previously, items are simply containers for data: an Image represents an image, and Keypoints some keypoints.","category":"page"},{"location":"iteminterface/#Why-do-I-need-to-wrap-my-data-in-an-item?","page":"Item Interface","title":"Why do I need to wrap my data in an item?","text":"","category":"section"},{"location":"iteminterface/","page":"Item Interface","title":"Item Interface","text":"For one, the item structs may contain metadata that is useful for some transformations. More importantly, though, by wrapping data in an item type, the meaning of the data is separated from the representation, that is, the concrete type.","category":"page"},{"location":"iteminterface/","page":"Item Interface","title":"Item Interface","text":"An Array{Integer, 2} could represent an image, but also a multi-class segmentation mask. Is Array{Float32, 3} a 3-dimensional image or a 2-dimensional image with the color channels expanded?","category":"page"},{"location":"iteminterface/","page":"Item Interface","title":"Item Interface","text":"Separating the representation from the data's meaning resolves those ambiguities.","category":"page"},{"location":"iteminterface/#Creating-items","page":"Item Interface","title":"Creating items","text":"","category":"section"},{"location":"iteminterface/","page":"Item Interface","title":"Item Interface","text":"To create a new item, you can simply subtype Item:","category":"page"},{"location":"iteminterface/","page":"Item Interface","title":"Item Interface","text":"struct MyItem <: Item\n    data\nend","category":"page"},{"location":"iteminterface/","page":"Item Interface","title":"Item Interface","text":"The only function that is expected to be implemented is itemdata, which simply returns the wrapped data. If, as above, you simply call the field holding the data data, you do not need to implement it. The same goes for the DataAugmentation.setdata helper.","category":"page"},{"location":"iteminterface/","page":"Item Interface","title":"Item Interface","text":"For some items, it also makes sense to implement the following:","category":"page"},{"location":"iteminterface/","page":"Item Interface","title":"Item Interface","text":"DataAugmentation.showitem!(img, item::I) creates a visual representation of an item on top of img.","category":"page"},{"location":"ref/#Reference","page":"References","title":"Reference","text":"","category":"section"},{"location":"ref/","page":"References","title":"References","text":"AdjustBrightness\nAdjustContrast\nBoundingBox\nCenterCrop\nCenterResizeCrop\nCrop\nFlipX\nFlipY\nImage\nKeypoints\nMaskBinary\nMaskMulti\nMaybe\nOneOf\nPermuteDims\nPolygon\nRandomCrop\nRandomResizeCrop\nReflect\nRotate\nRotateX\nRotateY\nRotateZ\nScaleKeepAspect\nScaleRatio\nWarpAffine\nitemdata\nshowitems\n\nDataAugmentation.AbstractArrayItem\nDataAugmentation.AbstractItem\nDataAugmentation.ArrayItem\nDataAugmentation.Buffered\nDataAugmentation.BufferedThreadsafe\nDataAugmentation.Categorify\nDataAugmentation.ComposedProjectiveTransform\nDataAugmentation.FillMissing\nDataAugmentation.Identity\nDataAugmentation.ImageToTensor\nDataAugmentation.Item\nDataAugmentation.ItemWrapper\nDataAugmentation.MapElem\nDataAugmentation.Normalize\nDataAugmentation.NormalizeRow\nDataAugmentation.OneHot\nDataAugmentation.PinOrigin\nDataAugmentation.ProjectiveTransform\nDataAugmentation.ResizePadDivisible \nDataAugmentation.ScaleFixed\nDataAugmentation.Sequence\nDataAugmentation.ToEltype\nDataAugmentation.Transform\nDataAugmentation.Zoom\nDataAugmentation.apply \nDataAugmentation.apply! \nDataAugmentation.boundsof \nDataAugmentation.centered \nDataAugmentation.compose\nDataAugmentation.getbounds\nDataAugmentation.getprojection\nDataAugmentation.getrandstate \nDataAugmentation.makebuffer \nDataAugmentation.offsetcropbounds \nDataAugmentation.project\nDataAugmentation.project! \nDataAugmentation.projectionbounds \nDataAugmentation.setdata\nDataAugmentation.showitem!\nDataAugmentation.testapply \nDataAugmentation.testapply! \nDataAugmentation.testitem\nDataAugmentation.testprojective \nDataAugmentation.threepointwarpaffine \nDataAugmentation.transformbounds ","category":"page"},{"location":"ref/#DataAugmentation.AdjustBrightness","page":"References","title":"DataAugmentation.AdjustBrightness","text":"AdjustBrightness(δ = 0.2)\nAdjustBrightness(distribution)\n\nAdjust the brightness of an image by a factor chosen uniformly from f ∈ [1-δ, 1+δ] by multiplying each color channel by f.\n\nYou can also pass any Distributions.Sampleable from which the factor is selected.\n\nPixels are clamped to [0,1] unless clamp=false is passed.\n\nExample\n\nusing DataAugmentation, TestImages\n\nitem = Image(testimage(\"lighthouse\"))\ntfm = AdjustBrightness(0.2)\ntitems = [apply(tfm, item) for _ in 1:8]\nshowgrid(titems; ncol = 4, npad = 16)\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.AdjustContrast","page":"References","title":"DataAugmentation.AdjustContrast","text":"AdjustContrast(factor = 0.2)\nAdjustContrast(distribution)\n\nAdjust the contrast of an image by a factor chosen uniformly from f ∈ [1-δ, 1+δ].\n\nPixels c are transformed c + μ*(1-f) where μ is the mean color of the image.\n\nYou can also pass any Distributions.Sampleable from which the factor is selected.\n\nPixels are clamped to [0,1] unless clamp=false is passed.\n\nExample\n\nusing DataAugmentation, TestImages\n\nitem = Image(testimage(\"lighthouse\"))\ntfm = AdjustContrast(0.2)\ntitems = [apply(tfm, item) for _ in 1:8]\nshowgrid(titems; ncol = 4, npad = 16)\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.BoundingBox","page":"References","title":"DataAugmentation.BoundingBox","text":"BoundingBox(points, sz)\nBoundingBox{N, T, M}(points, bounds)\n\nItem wrapper around Keypoints.\n\nExamples\n\n{cell=BoundingBox}\n\nusing DataAugmentation, StaticArrays\npoints = [SVector(10., 10.), SVector(80., 60.)]\nitem = BoundingBox(points, (100, 100))\n\n{cell=BoundingBox}\n\nshowitems(item)\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.CenterCrop","page":"References","title":"DataAugmentation.CenterCrop","text":"Crop(sz, FromCenter())\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.CenterResizeCrop","page":"References","title":"DataAugmentation.CenterResizeCrop","text":"ScaleKeepAspect(sz) |> CenterCrop(sz) |> PinOrigin()\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.Crop","page":"References","title":"DataAugmentation.Crop","text":"Crop(sz, FromOrigin())\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.FlipX","page":"References","title":"DataAugmentation.FlipX","text":"Reflect(180)\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.FlipY","page":"References","title":"DataAugmentation.FlipY","text":"Reflect(90)\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.Image","page":"References","title":"DataAugmentation.Image","text":"Image(image[, bounds])\n\nItem representing an N-dimensional image with element type T.\n\nExamples\n\nusing DataAugmentation, Images\n\nimagedata = rand(RGB, 100, 100)\nitem = Image(imagedata)\nshowitems(item)\n\nIf T is not a color, the image will be interpreted as grayscale:\n\nimagedata = rand(Float32, 100, 100)\nitem = Image(imagedata)\nshowitems(item)\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.Keypoints","page":"References","title":"DataAugmentation.Keypoints","text":"Keypoints(points, sz)\nKeypoints{N, T, M}(points, bounds)\n\nN-dimensional keypoints represented as SVector{N, T}.\n\nSpatial bounds are given by the polygon bounds::Vector{SVector{N, T}} or sz::NTuple{N, Int}.\n\nExamples\n\n{cell=Keypoints}\n\nusing DataAugmentation, StaticArrays\npoints = [SVector(y, x) for (y, x) in zip(4:5:80, 10:6:90)]\nitem = Keypoints(points, (100, 100))\n\n{cell=Keypoints}\n\nshowitems(item)\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.MaskBinary","page":"References","title":"DataAugmentation.MaskBinary","text":"MaskBinary(a)\n\nAn N-dimensional binary mask.\n\nExamples\n\n{cell=MaskMulti}\n\nusing DataAugmentation\n\nmask = MaskBinary(rand(Bool, 100, 100))\n\n{cell=MaskMulti}\n\nshowitems(mask)\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.MaskMulti","page":"References","title":"DataAugmentation.MaskMulti","text":"MaskMulti(a, [classes])\n\nAn N-dimensional multilabel mask with labels classes.\n\nExamples\n\n{cell=MaskMulti}\n\nusing DataAugmentation\n\nmask = MaskMulti(rand(1:3, 100, 100))\n\n{cell=MaskMulti}\n\nshowitems(mask)\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.Maybe","page":"References","title":"DataAugmentation.Maybe","text":"Maybe(tfm, p = 0.5) <: Transform\n\nWith probability p, apply transformation tfm.\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.OneOf","page":"References","title":"DataAugmentation.OneOf","text":"OneOf(tfms)\nOneOf(tfms, ps)\n\nApply one of tfms selected randomly with probability ps each or uniformly chosen if no ps is given.\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.PermuteDims","page":"References","title":"DataAugmentation.PermuteDims","text":"PermuteDims(perm)\n\nPermute the dimensions of an ArrayItem. perm is a vector or a tuple specifying the permutation, whose length has to match the dimensionality of the ArrayItems data.\n\nRefer to the permutedims documentation for examples of permutation vectors perm.\n\nSupports apply!.\n\nExamples\n\nPreprocessing an image with 3 color channels.\n\n{cell=PermuteDims}\n\nusing DataAugmentation, Images\nimage = Image(rand(RGB, 20, 20))\n\n# Turn image to tensor and permute dimensions 2 and 1\n# to convert HWC (height, width, channel) array to WHC (width, height, channel)\ntfms = ImageToTensor() |> PermuteDims(2, 1, 3)\napply(tfms, image)\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.Polygon","page":"References","title":"DataAugmentation.Polygon","text":"Polygon(points, sz)\nPolygon{N, T, M}(points, bounds)\n\nItem wrapper around Keypoints.\n\nExamples\n\n{cell=Polygon}\n\nusing DataAugmentation, StaticArrays\npoints = [SVector(10., 10.), SVector(80., 20.), SVector(90., 70.), SVector(20., 90.)]\nitem = Polygon(points, (100, 100))\n\n{cell=Polygon}\n\nshowitems(item)\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.RandomCrop","page":"References","title":"DataAugmentation.RandomCrop","text":"Crop(sz, FromRandom())\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.RandomResizeCrop","page":"References","title":"DataAugmentation.RandomResizeCrop","text":"ScaleKeepAspect(sz) |> RandomCrop(sz) |> PinOrigin()\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.Reflect","page":"References","title":"DataAugmentation.Reflect","text":"Reflect(γ)\nReflect(distribution)\n\nReflect 2D spatial data around the center by an angle chosen at uniformly from [-γ, γ], an angle given in degrees.\n\nYou can also pass any Distributions.Sampleable from which the angle is selected.\n\nExamples\n\ntfm = Reflect(10)\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.Rotate","page":"References","title":"DataAugmentation.Rotate","text":"Rotate(γ)\nRotate(distribution)\nRotate(α, β, γ)\nRotate(α_distribution, β_distribution, γ_distribution)\n\nRotate spatial data around its center. Rotate(γ) is a 2D rotation by an angle chosen uniformly from [-γ, γ], an angle given in degrees. Rotate(α, β, γ) is a 3D rotation by angles chosen uniformly from [-α, α], [-β, β], and [-γ, γ], for X, Y, and Z rotations.\n\nYou can also pass any Distributions.Sampleable from which the angle is selected.\n\nExamples\n\ntfm2d = Rotate(10)\napply(tfm2d, Image(rand(Float32, 16, 16)))\n\ntfm3d = Rotate(10, 20, 30)\napply(tfm3d, Image(rand(Float32, 16, 16, 16)))\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.RotateX","page":"References","title":"DataAugmentation.RotateX","text":"RotateX(γ)\nRotateX(distribution)\n\nX-Axis rotation of 3D spatial data around the center by an angle chosen uniformly from [-γ, γ], an angle given in degrees.\n\nYou can also pass any Distributions.Sampleable from which the angle is selected.\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.RotateY","page":"References","title":"DataAugmentation.RotateY","text":"RotateY(γ)\nRotateY(distribution)\n\nY-Axis rotation of 3D spatial data around the center by an angle chosen uniformly from [-γ, γ], an angle given in degrees.\n\nYou can also pass any Distributions.Sampleable from which the angle is selected.\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.RotateZ","page":"References","title":"DataAugmentation.RotateZ","text":"RotateZ(γ)\nRotateZ(distribution)\n\nZ-Axis rotation of 3D spatial data around the center by an angle chosen uniformly from [-γ, γ], an angle given in degrees.\n\nYou can also pass any Distributions.Sampleable from which the angle is selected.\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.ScaleKeepAspect","page":"References","title":"DataAugmentation.ScaleKeepAspect","text":"ScaleKeepAspect(minlengths) <: ProjectiveTransform\n\nScales the shortest side of item to minlengths, keeping the original aspect ratio.\n\nExamples\n\nusing DataAugmentation, TestImages\nimage = testimage(\"lighthouse\")\ntfm = ScaleKeepAspect((200, 200))\napply(tfm, Image(image))\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.ScaleRatio","page":"References","title":"DataAugmentation.ScaleRatio","text":"ScaleRatio(minlengths) <: ProjectiveTransform\n\nScales the aspect ratio\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.WarpAffine","page":"References","title":"DataAugmentation.WarpAffine","text":"WarpAffine(σ = 0.1) <: ProjectiveTransform\n\nA three-point affine warp calculated by randomly moving 3 corners of an item. Similar to a random translation, shear and rotation.\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.itemdata","page":"References","title":"DataAugmentation.itemdata","text":"itemdata(item)\nitemdata(items)\n\nAccess the data wrapped in item or a tuple of items.\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.showitems","page":"References","title":"DataAugmentation.showitems","text":"showitems(items)\n\nVisualize items.\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.AbstractArrayItem","page":"References","title":"DataAugmentation.AbstractArrayItem","text":"abstract type AbstractArrayItem{N, T}\n\nAbstract type for all [Item]s that wrap an N-dimensional array with element type T.\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.AbstractItem","page":"References","title":"DataAugmentation.AbstractItem","text":"abstract type AbstractItem\n\nAbstract supertype for all items. To implement items, subtype either Item to create a new item or ItemWrapper to wrap an existing item.\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.ArrayItem","page":"References","title":"DataAugmentation.ArrayItem","text":"ArrayItem(a)\n\nAn item that contains an array.\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.Buffered","page":"References","title":"DataAugmentation.Buffered","text":"Buffer to store transform results\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.BufferedThreadsafe","page":"References","title":"DataAugmentation.BufferedThreadsafe","text":"Buffer to store transform results (threadsafe)\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.Categorify","page":"References","title":"DataAugmentation.Categorify","text":"Categorify(dict, cols)\n\nLabel encodes the values of a row present in TabularItem for the  columns specified in cols using dict, which contains the column  names as dictionary keys and the unique values of column present  as dictionary values.\n\nif there are any missing values in the values to be transformed,  they are replaced by 1.\n\nExample\n\nusing DataAugmentation\n\ncols = [:col1, :col2, :col3]\nrow = (; zip(cols, [\"cat\", 2, 3])...)\nitem = TabularItem(row, cols)\ncatdict = Dict(:col1 => [\"dog\", \"cat\"])\n\ntfm = Categorify(catdict, [:col1])\napply(tfm, item)\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.ComposedProjectiveTransform","page":"References","title":"DataAugmentation.ComposedProjectiveTransform","text":"ComposedProjectiveTransform(tfms...)\n\nWrap multiple projective tfms and apply them efficiently. The projections are fused into a single projection and only points inside the final crop are evaluated.\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.FillMissing","page":"References","title":"DataAugmentation.FillMissing","text":"FillMissing(dict, cols)\n\nFills the missing values of a row present in TabularItem for the columns  specified in cols using dict, which contains the column names as  dictionary keys and the value to fill the column with present as  dictionary values.\n\nExample\n\nusing DataAugmentation\n\ncols = [:col1, :col2, :col3]\nrow = (; zip(cols, [1, 2, 3])...)\nitem = TabularItem(row, cols)\nfmdict = Dict(:col1 => 100, :col2 => 100)\n\ntfm = FillMissing(fmdict, [:col1, :col2])\napply(tfm, item)\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.Identity","page":"References","title":"DataAugmentation.Identity","text":"Identity()\n\nThe identity transformation.\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.ImageToTensor","page":"References","title":"DataAugmentation.ImageToTensor","text":"ImageToTensor()\n\nExpands an Image{N, T} of size (height, width, ...) to an ArrayItem{N+1} with size (width, height, ..., ch) where ch is the number of color channels of T.\n\nSupports apply!.\n\nExamples\n\n{cell=ImageToTensor}\n\nusing DataAugmentation, Images\n\nh, w = 40, 50\nimage = Image(rand(RGB, h, w))\ntfm = ImageToTensor()\napply(tfm, image) # ArrayItem in WHC format of size (50, 40, 3)\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.Item","page":"References","title":"DataAugmentation.Item","text":"abstract type Item\n\nAbstract supertype of concrete items.\n\nSubtype if you want to create a new item. If you want to wrap an existing item, see ItemWrapper.\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.ItemWrapper","page":"References","title":"DataAugmentation.ItemWrapper","text":"abstract type ItemWrapper{Item}\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.MapElem","page":"References","title":"DataAugmentation.MapElem","text":"MapElem(f)\n\nApplies f to every element in an [AbstractArrayItem].\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.Normalize","page":"References","title":"DataAugmentation.Normalize","text":"Normalize(means, stds)\n\nNormalizes the last dimension of an AbstractArrayItem{N}.\n\nSupports apply!.\n\nExamples\n\nPreprocessing a 3D image with 3 color channels.\n\n{cell=Normalize}\n\nusing DataAugmentation, Images\nimage = Image(rand(RGB, 20, 20, 20))\ntfms = ImageToTensor() |> Normalize((0.1, -0.2, -0.1), (1,1,1.))\napply(tfms, image)\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.NormalizeRow","page":"References","title":"DataAugmentation.NormalizeRow","text":"NormalizeRow(dict, cols)\n\nNormalizes the values of a row present in TabularItem for the columns  specified in cols using dict, which contains the column names as  dictionary keys and the mean and standard deviation tuple present as  dictionary values.\n\nExample\n\nusing DataAugmentation\n\ncols = [:col1, :col2, :col3]\nrow = (; zip(cols, [1, 2, 3])...)\nitem = TabularItem(row, cols)\nnormdict = Dict(:col1 => (1, 1), :col2 => (2, 2))\n\ntfm = NormalizeRow(normdict, [:col1, :col2])\napply(tfm, item)\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.OneHot","page":"References","title":"DataAugmentation.OneHot","text":"OneHot([T = Float32])\n\nOne-hot encodes a MaskMulti with n classes and size sz into an array item of size (sz..., n) with element type T. Supports apply!.\n\nitem = MaskMulti(rand(1:4, 100, 100), 1:4)\napply(OneHot(), item)\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.PinOrigin","page":"References","title":"DataAugmentation.PinOrigin","text":"PinOrigin()\n\nProjective transformation that translates the data so that the upper left bounding corner is at the origin (0, 0) (or the multidimensional equivalent).\n\nProjective transformations on images return OffsetArrays, but not on keypoints. Hardware like GPUs do not support OffsetArrays, so they will be unwrapped and no longer match up with the keypoints.\n\nPinning the data to the origin makes sure that the resulting OffsetArray has the same indices as a regular array, starting at one.\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.ProjectiveTransform","page":"References","title":"DataAugmentation.ProjectiveTransform","text":"abstract type ProjectiveTransform <: Transform\n\nAbstract supertype for projective transformations. See\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.ResizePadDivisible","page":"References","title":"DataAugmentation.ResizePadDivisible","text":"ScaleKeepAspect(sz) |> PadDivisible(by) |> PinOrigin()\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.ScaleFixed","page":"References","title":"DataAugmentation.ScaleFixed","text":"ScaleFixed(sizes)\n\nProjective transformation that scales sides to sizes, disregarding aspect ratio.\n\nSee also ScaleKeepAspect.\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.Sequence","page":"References","title":"DataAugmentation.Sequence","text":"Sequence(transforms...)\n\nTransform that applies multiple transformations after each other.\n\nYou should not use this explicitly. Instead use compose.\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.ToEltype","page":"References","title":"DataAugmentation.ToEltype","text":"ToEltype(T)\n\nConverts any AbstractArrayItem to an AbstractArrayItem{N, T}.\n\nSupports apply!.\n\nExamples\n\n{cell=ToEltype}\n\nusing DataAugmentation\n\ntfm = ToEltype(Float32)\nitem = ArrayItem(rand(Int, 10))\napply(tfm, item)\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.Transform","page":"References","title":"DataAugmentation.Transform","text":"abstract type Transform\n\nAbstract supertype for all transformations.\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.Zoom","page":"References","title":"DataAugmentation.Zoom","text":"Zoom(scales = (1, 1.2)) <: ProjectiveTransform\nZoom(distribution)\n\nZoom into an item by a factor chosen from the interval scales or distribution.\n\n\n\n\n\n","category":"type"},{"location":"ref/#DataAugmentation.apply","page":"References","title":"DataAugmentation.apply","text":"apply(tfm, item[; randstate])\napply(tfm, items[; randstate])\n\nApply tfm to an item or a tuple items.\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.apply!","page":"References","title":"DataAugmentation.apply!","text":"apply!(buffer::I, tfm, item::I)\n\nApplies tfm to item, mutating the preallocated buffer.\n\nbuffer can be obtained with buffer = makebuffer(tfm, item)\n\napply!(buffer, tfm::Transform, item::I; randstate) = apply(tfm, item; randstate)\n\nDefault to apply(tfm, item) (non-mutating version).\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.boundsof","page":"References","title":"DataAugmentation.boundsof","text":"boundingranges(ps)\n\nFind bounding index ranges for points ps.\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.centered","page":"References","title":"DataAugmentation.centered","text":"centered(P, bounds)\n\nTransform P so that is applied around the center of bounds instead of the origin\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.compose","page":"References","title":"DataAugmentation.compose","text":"compose(transforms...)\n\nCompose tranformations. Use |> as an alias.\n\nDefaults to creating a Sequence of transformations, but smarter behavior can be implemented. For example, MapElem(f) |> MapElem(g) == MapElem(g ∘ f).\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.getbounds","page":"References","title":"DataAugmentation.getbounds","text":"getbounds(item)\n\nReturn the spatial bounds of item. For a 2D-image (Image{2}) the bounds are the 4 corners of the bounding rectangle. In general, for an N-dimensional item, the bounds are a vector of the N^2 corners of the N-dimensional hypercube bounding the data.\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.getprojection","page":"References","title":"DataAugmentation.getprojection","text":"getprojection(tfm, bounds; randstate)\n\nCreate a projection for an item with spatial bounds bounds. The projection should be a CoordinateTransformations.Transformation. See CoordinateTransformations.jl\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.getrandstate","page":"References","title":"DataAugmentation.getrandstate","text":"getrandstate(transform)\n\nGenerates random state for stochastic transformations. Calling apply(tfm, item) is equivalent to apply(tfm, item; randstate = getrandstate(tfm)). It defaults to nothing, so you it only needs to be implemented for stochastic Transforms.\n\n\n\n\n\nReturn random state of the transform\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.makebuffer","page":"References","title":"DataAugmentation.makebuffer","text":"makebuffer(tfm, item)\n\nCreate a buffer buf that can be used in a call to apply!(buf, tfm, item). Default to buffer = apply(tfm, item).\n\nYou only need to implement this if the default apply(tfm, item) isn't enough. See apply(tfm::Sequence, item) for an example of this.\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.offsetcropbounds","page":"References","title":"DataAugmentation.offsetcropbounds","text":"offsetcropbounds(sz, bounds, offsets)\n\nCalculate offset bounds for a crop of size sz.\n\nFor every dimension i where sz[i] < length(indices[i]), offsets the crop by offsets[i] times the difference between the two.\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.project","page":"References","title":"DataAugmentation.project","text":"project(P, item, indices)\n\nProject item using projection P and crop to indices if given.\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.project!","page":"References","title":"DataAugmentation.project!","text":"project!(bufitem, P, item, indices)\n\nProject item using projection P and crop to indices if given. Store result in bufitem. Inplace version of project.\n\nDefault implementation falls back to project.\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.projectionbounds","page":"References","title":"DataAugmentation.projectionbounds","text":"projectionbounds(tfm, P, bounds, randstate)\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.setdata","page":"References","title":"DataAugmentation.setdata","text":"Provides a convenient way to create a copy of an item, replacing only the wrapped data. This relies on the wrapped data field being named data, though.\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.showitem!","page":"References","title":"DataAugmentation.showitem!","text":"showitem!(item)\n\nVisualize item. Should return an image.\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.testapply","page":"References","title":"DataAugmentation.testapply","text":"testapply(tfm, item)\ntestapply(tfm, I)\n\nTest apply invariants of tfm on item or item type I.\n\nWith a constant randstate parameter, apply should always return the  same result.\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.testapply!","page":"References","title":"DataAugmentation.testapply!","text":"testapply!(tfm, Items)\ntestapply!(tfm, Item)\ntestapply!(tfm, item1, item2)\n\nTest apply! invariants.\n\nWith a constant randstate parameter, apply! should always return the  same result.\nGiven a different item than was used to create the buffer, the buffer's data  should be modified.\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.testitem","page":"References","title":"DataAugmentation.testitem","text":"testitem(TItem)\n\nCreate an instance of an item with type TItem. If it has spatial bounds, should return an instance with bounds with ranges (1:16, 1:16).\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.testprojective","page":"References","title":"DataAugmentation.testprojective","text":"testprojective(tfm)\n\nTest invariants of a ProjectiveTransform.\n\ngetprojection is defined, and, given a constant randstate parameter,  always returns the same result.\nIt preserves the item type, i.e. apply(tfm, ::I) -> I.\nApplying it to multiple items with the same bounds results in the same bounds  for all items.\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.threepointwarpaffine","page":"References","title":"DataAugmentation.threepointwarpaffine","text":"threepointwarpaffine(srcps, dstps)\n\nCalculate an affine CoordinateTransformations.LinearMap from 3 source points to 3 destination points.\n\nAdapted from  CoordinateTransformations.jl#30.\n\n\n\n\n\n","category":"function"},{"location":"ref/#DataAugmentation.transformbounds","page":"References","title":"DataAugmentation.transformbounds","text":"transformbounds(bounds, P)\n\nApply CoordinateTransformations.Transformation to bounds.\n\n\n\n\n\n","category":"function"},{"location":"#DataAugmentation.jl","page":"DataAugmentation.jl","title":"DataAugmentation.jl","text":"","category":"section"},{"location":"","page":"DataAugmentation.jl","title":"DataAugmentation.jl","text":"This library provides data transformations for machine and deep learning. At the moment, it focuses on spatial data (think images, keypoint data and masks), but that is owed only to my current work. The extensible abstractions should fit other domains as well.","category":"page"},{"location":"","page":"DataAugmentation.jl","title":"DataAugmentation.jl","text":"For the most part, the transformations themselves are not very complex. The challenge this library tackles is to reconcile an easy-to-use, composable interface with performant execution.","category":"page"},{"location":"","page":"DataAugmentation.jl","title":"DataAugmentation.jl","text":"The key abstractions are Transforms, the transformation to apply, and Items which contain the data to be transformed. apply(tfm, item), as the name gives away, applies a transformation to an item.  For example, given an Image item, we can resize it with the CenterResizeCrop transformation.","category":"page"},{"location":"","page":"DataAugmentation.jl","title":"DataAugmentation.jl","text":"item = Image(image)\ntfm = CenterResizeCrop((128, 128))\napply(tfm, item)","category":"page"},{"location":"#Requirements","page":"DataAugmentation.jl","title":"Requirements","text":"","category":"section"},{"location":"","page":"DataAugmentation.jl","title":"DataAugmentation.jl","text":"The above example is simple, but there are more requirements of data augmentation pipelines that this library adresses. They serve as a motivation to the interface I've arrived at for defining transformations.","category":"page"},{"location":"#Stochasticity","page":"DataAugmentation.jl","title":"Stochasticity","text":"","category":"section"},{"location":"","page":"DataAugmentation.jl","title":"DataAugmentation.jl","text":"A transformation is stochastic (as opposed to deterministic) if it produces different outputs based on some random state. This randomness can become a problem when applying an transformation to an aligned pair of input and target. If we have an image and a corresponding segmentation mask, using different scaling factors results in misalignment of the two; the segmentation no longer matches up with the image pixels.","category":"page"},{"location":"","page":"DataAugmentation.jl","title":"DataAugmentation.jl","text":"To handle this, the random state is explicitly passed to the transformations, rendering them deterministic. A generator for the random state can be defined with DataAugmentation.getrandstate(tfm) and passed to apply with the randstate keyword argument.","category":"page"},{"location":"#Composition","page":"DataAugmentation.jl","title":"Composition","text":"","category":"section"},{"location":"","page":"DataAugmentation.jl","title":"DataAugmentation.jl","text":"Most data augmentation pipelines are made up of multiple steps: augmenting an image can mean resizing, randomly rotating, cropping and then normalizing the values. So applying transformations one after another – sequencing – is one way to compose transformations. But some operations, like affine transformations, can also be fused, resulting in a single transformation that is more performant and produces more accurate results.","category":"page"},{"location":"#Buffering","page":"DataAugmentation.jl","title":"Buffering","text":"","category":"section"},{"location":"","page":"DataAugmentation.jl","title":"DataAugmentation.jl","text":"Since data augmentation pipelines often run on large amounts of data, performance can often be improved by using prealloacted output buffers for the transformations. This results in fewer memory allocations and less garbage collection which both take time. ","category":"page"},{"location":"","page":"DataAugmentation.jl","title":"DataAugmentation.jl","text":"","category":"page"},{"location":"","page":"DataAugmentation.jl","title":"DataAugmentation.jl","text":"Let's next see how these requirements are reflected in the item and transformation interfaces.","category":"page"}]
}
