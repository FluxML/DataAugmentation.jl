<HTML><head><title>Projective transformations</title><link href=../../../template/hugobook.css rel=stylesheet ></link><meta content=Type=text/html; charset=utf-8 http-equiv=Content-Type ></meta></head><body><input onclick=toggleMenu() id=menu-control class=hidden toggle type=checkbox ></input><input id=toc-control type=checkbox class=hidden toggle ></input><main class=container flex ><aside id=menu-container class=book-menu ><nav class=book-menu-content ><h2 id=title >DataAugmentation.jl</h2><div id=sidebar ><div class=doctree ><body><ul><li><p>User guide (TODO)</p><ul><li><p><a href=../quickstart.md.html title= >Quickstart</a></p></li></ul></li><li><p>Library</p><ul><li><p><a href=../intro.md.html title= >Introduction</a></p></li><li><p><a href=../iteminterface.md.html title= >Item interface</a></p></li><li><p><a href=../tfminterface.md.html title= >Transformation interface</a></p></li><li><p><a href=../buffering.md.html title= >Buffering</a></p></li><li><p>Projective transformations</p><ul><li><p><a href=intro.md.html title= >Introduction</a></p></li><li><p><a href=interface.md.html title= >Interface</a></p></li><li><p><a href=data.md.html title= >Spatial data</a></p></li><li><p><a href=usage.md.html title= >Usage</a></p></li><li><p><a href=gallery.md.html title= >Gallery</a></p></li></ul></li><li><p><a href=../preprocessing.md.html title= >Preprocessing</a></p></li></ul></li><li><p><a href=../../../REFERENCE.html title= >Reference</a></p></li></ul></body></div></div></nav></aside><div class=book-page ><header class=book-header ></header><article><h1 id=projective-transformations >Projective transformations</h1><p>DataAugmentation.jl has great support for transforming spatial data like images and keypoints. Most of these transformations are projective transformations. For our purposes, a projection means a mapping between two coordinate spaces. In computer vision, these are frequently used for preprocessing and augmenting image data: images are randomly scaled, maybe flipped horizontally and finally cropped to the same size.</p><p>This library generalizes projective transformations for different kinds of image and keypoint data in an N-dimensional Euclidean space. It also uses composition for performance improvements like fusing affine transformations.</p><p>Unlike mathematical objects, the spatial data we want to transform has <em>spatial bounds</em>. For an image, these bounds are akin to the array size. But keypoint data aligned with an image has the same bounds even if they are not explicitly encoded in the representation of the data.<br></br>These spatial bounds can be used to dynamically create useful transformations. For example, a rotation around the center or a horizontal flip of keypoint annotations can be calculated from the bounds.</p><p>Often, we also want to <em>crop</em> an area from the projected results. By evaluating only the parts of a projection that fall inside the cropped area, a lot of unnecessary computation can be avoided.</p><h2 id=an-example-pipeline >An example pipeline</h2><p>We can break down most augmentation used in practive into a single (possibly stochastic) projection and a crop.</p><p>As an example, consider an image augmentation pipeline: A random horizontal flip, followed by a random resized crop. The latter resizes and crops (irregularly sized) images to a common size without distorting the aspect ratio.</p><pre lang=julia ><code>Maybe(FlipX()) |&gt; RandomResizeCrop((h, w))
</code></pre><p>Let’s pull apart the steps involved.</p><ul><li><p>Half of the time, flip the image horizontally.</p></li><li><p>Scale the image down without distortion so that the shorter side length is 128. With an input of size <code>(512, 256)</code> this result in scaling both dimensions by <code>1/2</code>, resulting in an image with side lengths <code>(256, 128)</code>.</p></li><li><p>Crop a random <code>(128, 128)</code> portion from that image. There is only “wiggle room” on the y-axis (which, by convention, is the first).</p></li></ul><p>All of these steps can be efficiently computed in one step with two tricks:</p><ul><li><p>Some projections like reflection, translation, scaling and rotation can be composed into a single projection matrix. This means in the above example we only need to apply one projection which represents both the flipping (a reflection) and the scaling. Especially in pipelines with many augmentation steps this avoids a lot of unnecessary computation.</p></li><li><p>In cases, where the result of the projection is cropped, we can save additional computing by only evaluating the parts that we want to keep.</p></li></ul><h2 id=cropping >Cropping</h2><p>By default, the bounds of a projected item will be chosen so they still encase all the data. So after applying a <code>Scale((2, 2))</code> to an <code>Image</code>, its bounds will also be scaled by 2. Sometimes, however, we want to crop a part of the projected output, for example so a number of images can later be batched into a single array. While the crop usually has a fixed size, the region to crop still needs to be chosen. For validation data (which should be transformed deterministically), a center crop is usually used. For training data, on the other hand, a random region is selected to add additional augmentation.</p><hr></hr><p>Read on to find out <a href=./interface.md.html title= >how projective transformations are implemented</a> or jump straight to the <a href=./usage.md.html title= >usage section</a>.</p></article><footer class=book-footer ></footer></div><aside class=book-toc ><nav id=toc class=book-toc-content ><ul><li><a href=#projective-transformations >Projective transformations</a><ul><li><a href=#an-example-pipeline >An example pipeline</a><ul></ul></li><li><a href=#cropping >Cropping</a><ul></ul></li></ul></li></ul></nav></aside></main></body></HTML>